{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDPMScheduler {\n",
       "  \"_class_name\": \"DDPMScheduler\",\n",
       "  \"_diffusers_version\": \"0.32.2\",\n",
       "  \"beta_end\": 0.02,\n",
       "  \"beta_schedule\": \"linear\",\n",
       "  \"beta_start\": 0.0001,\n",
       "  \"clip_sample\": true,\n",
       "  \"clip_sample_range\": 1.0,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"steps_offset\": 0,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null,\n",
       "  \"variance_type\": \"fixed_small\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sample = scheduler.step(noise=noise, timestep=timestep, sample=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]s/it]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'diffusers.schedulers.scheduling_flow_match_euler_discrete.FlowMatchEulerDiscreteScheduler'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "PEFT backend is required for this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m pipe.enable_vae_slicing()\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(pipe.scheduler.compatibles)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdark_fantasy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdark_fantasy_lora.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdark_fantasy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mA dog holding a sign that says hello world\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m image = pipe(\n\u001b[32m     24\u001b[39m     prompt,\n\u001b[32m     25\u001b[39m     height=\u001b[32m256\u001b[39m,   \u001b[38;5;66;03m# 降低分辨率\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     generator=torch.Generator(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m).manual_seed(\u001b[32m0\u001b[39m)\n\u001b[32m     31\u001b[39m ).images[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libs/python/anaconda3/envs/comfyui/lib/python3.12/site-packages/diffusers/loaders/lora_pipeline.py:1513\u001b[39m, in \u001b[36mFluxLoraLoaderMixin.load_lora_weights\u001b[39m\u001b[34m(self, pretrained_model_name_or_path_or_dict, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1489\u001b[39m \u001b[33;03mLoad LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and\u001b[39;00m\n\u001b[32m   1490\u001b[39m \u001b[33;03m`self.text_encoder`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1510\u001b[39m \u001b[33;03m        weights.\u001b[39;00m\n\u001b[32m   1511\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_PEFT_BACKEND:\n\u001b[32m-> \u001b[39m\u001b[32m1513\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPEFT backend is required for this method.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1515\u001b[39m low_cpu_mem_usage = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mlow_cpu_mem_usage\u001b[39m\u001b[33m\"\u001b[39m, _LOW_CPU_MEM_USAGE_DEFAULT_LORA)\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft_version(\u001b[33m\"\u001b[39m\u001b[33m>=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0.13.1\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: PEFT backend is required for this method."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "# 初始化前清空缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"/home/jiuxia/ftp_file/FLUX.1-dev\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# 启用所有优化措施\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "pipe.enable_attention_slicing(1)\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "print(pipe.scheduler.compatibles)\n",
    "pipe.load_lora_weights(\"dark_fantasy\", weight_name=\"dark_fantasy_lora.safetensors\", adapter_name=\"dark_fantasy\")\n",
    "\n",
    "prompt = \"A dog holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=256,   # 降低分辨率\n",
    "    width=256,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=128,  # 缩短序列长度\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev-lora.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
